# HR-Analytics

## ğŸ“ PySpark Project Steps

This guide outlines the steps followed in the project for data processing using PySpark in Databricks.

---

### ğŸ“Œ Step 1: Loading & Understanding Data
- âœ… Reading CSV files in Databricks  
- âœ… Checking schema and data types  

---

### ğŸ“Œ Step 2: Data Cleaning & Transformation
- âœ… Renaming columns for better clarity  
- âœ… Converting categorical values into numerical format  
- âœ… Handling NULL values using `fillna()`, `dropna()`  

---

### ğŸ“Œ Step 3: Working with DataFrames
- âœ… Using `filter()`, `sort()` operations  
- âœ… Performing column operations like `withColumn()`, `alias()`, and `cast()`  

---

### ğŸ“Œ Step 4: Advanced PySpark Functions
- âœ… Using `explode()`, `collect_list()`, `pivot()`, `when()`, `otherwise()`  
- âœ… String functions: `initcap()`, `upper()`, `lower()`  
- âœ… Date functions: `current_date()`, `datediff()`, `date_add()`, `year()`, `month()`  

---

### ğŸ“Œ Step 5: Joins & Data Merging
- âœ… Inner, Left, Right & Outer Joins  
- âœ… `union()` & `unionByName()` for combining datasets  

---

### ğŸ“Œ Step 6: Window Functions & Ranking
- âœ… Using `rank()`, `dense_rank()`, and cumulative sum  
- âœ… Partitioning and ordering data efficiently  

---

### ğŸ“Œ Step 7: User Defined Functions (UDFs)
- âœ… Writing custom functions in PySpark  
- âœ… Applying UDFs to transform and clean data  

---

### ğŸ“Œ Step 8: Writing & Saving Data
- âœ… Writing datasets in CSV, JSON, ORC, and Delta formats  
- âœ… Overwriting, appending, and handling errors while saving  

---
